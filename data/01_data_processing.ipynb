{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from anarci import anarci\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8191907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 20 parquet файлов\n",
      "Общий размер данных: (1227083, 11)\n",
      "Колонки: ['dataset', 'heavy_sequence', 'light_sequence', 'scfv', 'affinity_type', 'affinity', 'antigen_sequence', 'confidence', 'nanobody', 'metadata', 'processed_measurement']\n"
     ]
    }
   ],
   "source": [
    "def load_asd_data_with_pandas(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загружает все parquet файлы из папки asd в один pandas DataFrame\n",
    "\n",
    "    Args:\n",
    "        data_path: путь к папке с данными\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: объединенный DataFrame со всеми данными\n",
    "    \"\"\"\n",
    "    # Получаем все parquet файлы из папки\n",
    "    parquet_files = glob.glob(os.path.join(data_path, \"part-*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise ValueError(f\"Не найдено parquet файлов в папке {data_path}\")\n",
    "\n",
    "    print(f\"Найдено {len(parquet_files)} parquet файлов\")\n",
    "\n",
    "    # Загружаем все файлы в список DataFrame'ов\n",
    "    dataframes = []\n",
    "    for file_path in parquet_files:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Объединяем все DataFrame'ы в один\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    print(f\"Общий размер данных: {combined_df.shape}\")\n",
    "    print(f\"Колонки: {list(combined_df.columns)}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Загружаем данные\n",
    "agab_df = load_asd_data_with_pandas('./asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c705170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переводим affinity и processed_measurement во float где возможно\n",
    "agab_df = agab_df.copy()\n",
    "agab_df.loc[:, 'affinity'] = pd.to_numeric(agab_df['affinity'], errors='coerce').fillna(agab_df['affinity'])\n",
    "agab_df.loc[:, 'processed_measurement'] = pd.to_numeric(agab_df['processed_measurement'], errors='coerce').fillna(agab_df['processed_measurement'])\n",
    "type(agab_df['affinity'][agab_df['affinity_type'] == 'bool'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a2ab3",
   "metadata": {},
   "source": [
    "#### Base filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4353ef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844872, 11)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_not_nanobody = agab_df['nanobody'] == False\n",
    "# is_high_confidence = agab_df['confidence'].isin(['high', 'very_high'])\n",
    "is_scfv = agab_df['scfv'] == True\n",
    "has_both_chains = (\n",
    "    agab_df['light_sequence'].notna() \n",
    "    & agab_df['heavy_sequence'].notna()\n",
    "    & (agab_df['light_sequence'] != '')\n",
    "    & (agab_df['heavy_sequence'] != '')\n",
    ")\n",
    "\n",
    "agab_df = agab_df[is_not_nanobody \n",
    "# & is_high_confidence \n",
    "& (is_scfv | has_both_chains)]\n",
    "agab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "284e1d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_count</th>\n",
       "      <th>unique_antigens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affinity_type</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">bool</th>\n",
       "      <th>patents</th>\n",
       "      <td>21621</td>\n",
       "      <td>3478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structures-antibodies</th>\n",
       "      <td>2711</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genbank</th>\n",
       "      <td>98</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skempiv2</th>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddg</th>\n",
       "      <th>skempiv2</th>\n",
       "      <td>400</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elisa_mut_to_wt_ratio</th>\n",
       "      <th>abdesign</th>\n",
       "      <td>658</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bool</th>\n",
       "      <th>abdesign</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab-bind</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ic_50</th>\n",
       "      <th>dlgo</th>\n",
       "      <td>360</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddg</th>\n",
       "      <th>ab-bind</th>\n",
       "      <td>270</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log KD</th>\n",
       "      <th>abbd</th>\n",
       "      <td>152401</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <th>flab_hie2022</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alphaseq</th>\n",
       "      <th>alphaseq</th>\n",
       "      <td>131645</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <th>flab_rosace2023</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_enrichment</th>\n",
       "      <th>abbd</th>\n",
       "      <td>3452</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy</th>\n",
       "      <th>buzz</th>\n",
       "      <td>524346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">kd</th>\n",
       "      <th>flab_koenig2017</th>\n",
       "      <td>4275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flab_shanehsazzadeh2023</th>\n",
       "      <td>446</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flab_warszawski2019</th>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               total_count  unique_antigens\n",
       "affinity_type         dataset                                              \n",
       "bool                  patents                        21621             3478\n",
       "                      structures-antibodies           2711             1083\n",
       "                      genbank                           98               23\n",
       "                      skempiv2                          34               21\n",
       "ddg                   skempiv2                         400               19\n",
       "elisa_mut_to_wt_ratio abdesign                         658               13\n",
       "bool                  abdesign                          14               13\n",
       "                      ab-bind                           13               10\n",
       "ic_50                 dlgo                             360               10\n",
       "ddg                   ab-bind                          270                9\n",
       "-log KD               abbd                          152401                5\n",
       "kd                    flab_hie2022                      55                3\n",
       "alphaseq              alphaseq                      131645                2\n",
       "kd                    flab_rosace2023                   25                2\n",
       "log_enrichment        abbd                            3452                2\n",
       "fuzzy                 buzz                          524346                1\n",
       "kd                    flab_koenig2017                 4275                1\n",
       "                      flab_shanehsazzadeh2023          446                1\n",
       "                      flab_warszawski2019             2048                1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agab_df.groupby(['affinity_type', 'dataset'])['antigen_sequence'].agg([\n",
    "    ('total_count', 'count'),\n",
    "    ('unique_antigens', 'nunique')\n",
    "]).sort_values('unique_antigens', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8fd79b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После базовых фильтров: (844872, 11)\n",
      "Распределение по affinity_type до фильтрации по порогам:\n",
      "affinity_type\n",
      "fuzzy                    524346\n",
      "-log KD                  152401\n",
      "alphaseq                 131645\n",
      "bool                      24491\n",
      "kd                         6849\n",
      "log_enrichment             3452\n",
      "ddg                         670\n",
      "elisa_mut_to_wt_ratio       658\n",
      "ic_50                       360\n",
      "Name: count, dtype: int64\n",
      "\n",
      "После фильтрации по порогам аффинитета: (402392, 11)\n",
      "Распределение по affinity_type после фильтрации:\n",
      "affinity_type\n",
      "fuzzy                    172149\n",
      "-log KD                  141805\n",
      "alphaseq                  55670\n",
      "bool                      24491\n",
      "kd                         6744\n",
      "log_enrichment             1016\n",
      "ic_50                       349\n",
      "elisa_mut_to_wt_ratio       168\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (порог, оператор): '<', '>', '=='\n",
    "AFFINITY_THRESHOLDS = {\n",
    "    'fuzzy': ('h', '=='),\n",
    "    'bool': (1, '=='),\n",
    "    'alphaseq': (2, '<'),\n",
    "    '-log KD': (7, '>'),\n",
    "    'kd': (100, '<'),\n",
    "    'delta_g': (-9.5, '<'),\n",
    "    'log_enrichment': (1, '>'),\n",
    "    'elisa_mut_to_wt_ratio': (1, '>'),\n",
    "    'ic_50': (100, '<'),\n",
    "}\n",
    "\n",
    "def apply_affinity_filter(df: pd.DataFrame, thresholds: dict) -> pd.DataFrame:\n",
    "    masks = []\n",
    "    for affinity_type, (threshold, op) in thresholds.items():\n",
    "        type_mask = df['affinity_type'] == affinity_type\n",
    "        if op == '==':\n",
    "            mask = type_mask & (df['processed_measurement'] == threshold)\n",
    "        else:\n",
    "            numeric_affinity = pd.to_numeric(df['processed_measurement'], errors='coerce')\n",
    "            if op == '<':\n",
    "                mask = type_mask & (numeric_affinity < threshold)\n",
    "            else:\n",
    "                mask = type_mask & (numeric_affinity > threshold)\n",
    "        masks.append(mask)\n",
    "\n",
    "    if masks:\n",
    "        combined_mask = masks[0]\n",
    "        for mask in masks[1:]:\n",
    "            combined_mask = combined_mask | mask\n",
    "        return df[combined_mask]\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "print(f\"После базовых фильтров: {agab_df.shape}\")\n",
    "print(f\"Распределение по affinity_type до фильтрации по порогам:\")\n",
    "print(agab_df['affinity_type'].value_counts())\n",
    "\n",
    "# Применяем фильтрацию по порогам аффиности\n",
    "agab_df = apply_affinity_filter(agab_df, AFFINITY_THRESHOLDS)\n",
    "\n",
    "print(f\"\\nПосле фильтрации по порогам аффинитета: {agab_df.shape}\")\n",
    "print(f\"Распределение по affinity_type после фильтрации:\")\n",
    "print(agab_df['affinity_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3481668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_count</th>\n",
       "      <th>unique_antigens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affinity_type</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">bool</th>\n",
       "      <th>patents</th>\n",
       "      <td>21621</td>\n",
       "      <td>3478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structures-antibodies</th>\n",
       "      <td>2711</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genbank</th>\n",
       "      <td>98</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skempiv2</th>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdesign</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elisa_mut_to_wt_ratio</th>\n",
       "      <th>abdesign</th>\n",
       "      <td>168</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ic_50</th>\n",
       "      <th>dlgo</th>\n",
       "      <td>349</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bool</th>\n",
       "      <th>ab-bind</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log KD</th>\n",
       "      <th>abbd</th>\n",
       "      <td>141805</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <th>flab_hie2022</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alphaseq</th>\n",
       "      <th>alphaseq</th>\n",
       "      <td>55670</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <th>flab_rosace2023</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy</th>\n",
       "      <th>buzz</th>\n",
       "      <td>172149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">kd</th>\n",
       "      <th>flab_koenig2017</th>\n",
       "      <td>4275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flab_shanehsazzadeh2023</th>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flab_warszawski2019</th>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_enrichment</th>\n",
       "      <th>abbd</th>\n",
       "      <td>1016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               total_count  unique_antigens\n",
       "affinity_type         dataset                                              \n",
       "bool                  patents                        21621             3478\n",
       "                      structures-antibodies           2711             1083\n",
       "                      genbank                           98               23\n",
       "                      skempiv2                          34               21\n",
       "                      abdesign                          14               13\n",
       "elisa_mut_to_wt_ratio abdesign                         168               12\n",
       "ic_50                 dlgo                             349               10\n",
       "bool                  ab-bind                           13               10\n",
       "-log KD               abbd                          141805                5\n",
       "kd                    flab_hie2022                      55                3\n",
       "alphaseq              alphaseq                       55670                2\n",
       "kd                    flab_rosace2023                   25                2\n",
       "fuzzy                 buzz                          172149                1\n",
       "kd                    flab_koenig2017                 4275                1\n",
       "                      flab_shanehsazzadeh2023          341                1\n",
       "                      flab_warszawski2019             2048                1\n",
       "log_enrichment        abbd                            1016                1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agab_df.groupby(['affinity_type', 'dataset'])['antigen_sequence'].agg([\n",
    "    ('total_count', 'count'),\n",
    "    ('unique_antigens', 'nunique')\n",
    "]).sort_values('unique_antigens', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a3fc",
   "metadata": {},
   "source": [
    "#### Affinity variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23e1e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== АНАЛИЗ РАЗЛИЧИЙ В ПОЛЯХ ДЛЯ ДУБЛИКАТОВ ПО ПОСЛЕДОВАТЕЛЬНОСТЯМ (agab_df) ===\n",
      "\n",
      "Всего групп дубликатов: 64032\n",
      "\n",
      "Групп с различиями: 63348 (98.93%)\n",
      "Групп без различий: 684 (1.07%)\n",
      "\n",
      "Строк (записей) с различиями: 127356 (98.55%)\n",
      "Строк (записей) без различий: 1872 (1.45%)\n",
      "\n",
      "=== ПОЛЯ, КОТОРЫЕ ЧАЩЕ ВСЕГО РАЗЛИЧАЮТСЯ ===\n",
      "\n",
      "affinity: 63326 групп (99.97%)\n",
      "dataset: 22 групп (0.03%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== АНАЛИЗ РАЗЛИЧИЙ В ПОЛЯХ ДЛЯ ДУБЛИКАТОВ ПО ПОСЛЕДОВАТЕЛЬНОСТЯМ (agab_df) ===\\n\")\n",
    "\n",
    "# Поля для проверки\n",
    "fields = [\n",
    "    \"dataset\",\n",
    "    \"scfv\",\n",
    "    \"affinity_type\",\n",
    "    \"affinity\",\n",
    "    \"confidence\",\n",
    "    \"nanobody\",\n",
    "    \"processed_measurement\",\n",
    "]\n",
    "\n",
    "# --- Поиск групп дубликатов по heavy_sequence + light_sequence + antigen_sequence---\n",
    "# Находим группы, где хотя бы 2 записи с одинаковой парой последовательностей\n",
    "group_cols = [\"heavy_sequence\", \"light_sequence\", \"antigen_sequence\"]\n",
    "grouped = agab_df.groupby(group_cols, sort=False)\n",
    "\n",
    "# Маска строк, которые входят в группы размером > 1\n",
    "dup_mask = grouped[group_cols[0]].transform(\"size\") > 1\n",
    "hl_duplicates = agab_df.loc[dup_mask].copy()\n",
    "\n",
    "# Переиспользуем groupby только на дубликатах\n",
    "dup_groups = hl_duplicates.groupby(group_cols, sort=False)\n",
    "unique_groups_count = dup_groups.ngroups\n",
    "\n",
    "print(f\"Всего групп дубликатов: {unique_groups_count}\\n\")\n",
    "\n",
    "# --- Анализ различий по полям ---\n",
    "\n",
    "# Для каждой группы считаем, сколько уникальных значений в каждом поле\n",
    "# dropna=False, чтобы различия NaN / не-NaN тоже учитывались\n",
    "nunique_per_group = dup_groups[fields].nunique(dropna=False)\n",
    "\n",
    "# Булева матрица: True, если в группе по полю есть различия\n",
    "diff_mask = nunique_per_group > 1\n",
    "\n",
    "# Есть ли вообще различия в группе\n",
    "group_has_diffs = diff_mask.any(axis=1)\n",
    "\n",
    "groups_with_diffs = int(group_has_diffs.sum())\n",
    "groups_identical = int((~group_has_diffs).sum())\n",
    "total = groups_with_diffs + groups_identical if (groups_with_diffs + groups_identical) > 0 else 1\n",
    "\n",
    "print(f\"Групп с различиями: {groups_with_diffs} ({groups_with_diffs / total * 100:.2f}%)\")\n",
    "print(f\"Групп без различий: {groups_identical} ({groups_identical / total * 100:.2f}%)\\n\")\n",
    "\n",
    "# Количество строк (записей) с различиями и без различий\n",
    "rows_with_diffs = group_has_diffs[group_has_diffs].index  # индексы групп с различиями\n",
    "rows_without_diffs = group_has_diffs[~group_has_diffs].index  # индексы групп без различий\n",
    "\n",
    "num_rows_with_diffs = dup_groups.size().loc[rows_with_diffs].sum() if len(rows_with_diffs) > 0 else 0\n",
    "num_rows_without_diffs = dup_groups.size().loc[rows_without_diffs].sum() if len(rows_without_diffs) > 0 else 0\n",
    "rows_total = num_rows_with_diffs + num_rows_without_diffs if (num_rows_with_diffs + num_rows_without_diffs) > 0 else 1\n",
    "\n",
    "print(f\"Строк (записей) с различиями: {num_rows_with_diffs} ({num_rows_with_diffs / rows_total * 100:.2f}%)\")\n",
    "print(f\"Строк (записей) без различий: {num_rows_without_diffs} ({num_rows_without_diffs / rows_total * 100:.2f}%)\\n\")\n",
    "\n",
    "# Считаем, по скольким группам отличается каждое поле\n",
    "field_diffs_counts = diff_mask.sum().sort_values(ascending=False)\n",
    "\n",
    "if groups_with_diffs > 0 and not field_diffs_counts.empty:\n",
    "    print(\"=== ПОЛЯ, КОТОРЫЕ ЧАЩЕ ВСЕГО РАЗЛИЧАЮТСЯ ===\\n\")\n",
    "    for field, count in field_diffs_counts.items():\n",
    "        if count == 0:\n",
    "            continue\n",
    "        print(f\"{field}: {count} групп ({count / groups_with_diffs * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8e602e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== АНАЛИЗ РАЗБРОСА АФФИНИТЕТА В ГРУППАХ ДУБЛИКАТОВ ===\n",
      "\n",
      "Групп с валидными числовыми аффинитетами: 64032\n",
      "Средний разброс аффинитета в группе: 2.8970\n",
      "Медианный разброс: 3.2739\n",
      "Максимальный разброс: 3.8345\n",
      "\n",
      "Типы аффинитета внутри каждой группы совпадают (корректно сравнивать числа).\n",
      "\n",
      "=== ТОП-5 ГРУПП С САМЫМ БОЛЬШИМ РАЗБРОСОМ АФФИНИТЕТА ===\n",
      "\n",
      "Группа 1 (разброс 3.8345):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>affinity_type</th>\n",
       "      <th>affinity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>processed_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1021153</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>9.834508</td>\n",
       "      <td>high</td>\n",
       "      <td>7.917254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021154</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>high</td>\n",
       "      <td>7.917254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset affinity_type  affinity confidence processed_measurement\n",
       "1021153    abbd       -log KD  9.834508       high              7.917254\n",
       "1021154    abbd       -log KD       6.0       high              7.917254"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Группа 2 (разброс 3.7936):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>affinity_type</th>\n",
       "      <th>affinity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>processed_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409471</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>9.793633</td>\n",
       "      <td>high</td>\n",
       "      <td>7.896816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409472</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>high</td>\n",
       "      <td>7.896816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset affinity_type  affinity confidence processed_measurement\n",
       "409471    abbd       -log KD  9.793633       high              7.896816\n",
       "409472    abbd       -log KD       6.0       high              7.896816"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Группа 3 (разброс 3.7556):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>affinity_type</th>\n",
       "      <th>affinity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>processed_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471033</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>9.755628</td>\n",
       "      <td>high</td>\n",
       "      <td>7.877814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471034</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>high</td>\n",
       "      <td>7.877814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset affinity_type  affinity confidence processed_measurement\n",
       "471033    abbd       -log KD  9.755628       high              7.877814\n",
       "471034    abbd       -log KD       6.0       high              7.877814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Группа 4 (разброс 3.7512):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>affinity_type</th>\n",
       "      <th>affinity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>processed_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>591881</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>9.75118</td>\n",
       "      <td>high</td>\n",
       "      <td>7.87559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591882</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>high</td>\n",
       "      <td>7.87559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset affinity_type affinity confidence processed_measurement\n",
       "591881    abbd       -log KD  9.75118       high               7.87559\n",
       "591882    abbd       -log KD      6.0       high               7.87559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Группа 5 (разброс 3.7379):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>affinity_type</th>\n",
       "      <th>affinity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>processed_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>961707</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>9.737911</td>\n",
       "      <td>high</td>\n",
       "      <td>7.868956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961708</th>\n",
       "      <td>abbd</td>\n",
       "      <td>-log KD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>high</td>\n",
       "      <td>7.868956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset affinity_type  affinity confidence processed_measurement\n",
       "961707    abbd       -log KD  9.737911       high              7.868956\n",
       "961708    abbd       -log KD       6.0       high              7.868956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== АНАЛИЗ РАЗБРОСА АФФИНИТЕТА В ГРУППАХ ДУБЛИКАТОВ ===\\n\")\n",
    "\n",
    "# Работаем с копией дубликатов\n",
    "analysis_df = hl_duplicates.copy()\n",
    "\n",
    "# Преобразуем аффинитет в числа, нечисловые значения станут NaN\n",
    "analysis_df['affinity_numeric'] = pd.to_numeric(analysis_df['affinity'], errors='coerce')\n",
    "\n",
    "# Группируем\n",
    "grouped_analysis = analysis_df.groupby(group_cols, sort=False)\n",
    "\n",
    "# Считаем размах (max - min) и проверяем единственность типа аффинитета\n",
    "agg_funcs = {\n",
    "    'affinity_numeric': lambda x: x.max() - x.min(),\n",
    "    'affinity_type': 'nunique'\n",
    "}\n",
    "\n",
    "group_stats = grouped_analysis.agg(agg_funcs)\n",
    "group_stats.columns = ['affinity_range', 'affinity_type_count']\n",
    "\n",
    "# Фильтруем группы, где affinity_numeric удалось посчитать (не NaN)\n",
    "valid_stats = group_stats.dropna(subset=['affinity_range'])\n",
    "\n",
    "print(f\"Групп с валидными числовыми аффинитетами: {len(valid_stats)}\")\n",
    "print(f\"Средний разброс аффинитета в группе: {valid_stats['affinity_range'].mean():.4f}\")\n",
    "print(f\"Медианный разброс: {valid_stats['affinity_range'].median():.4f}\")\n",
    "print(f\"Максимальный разброс: {valid_stats['affinity_range'].max():.4f}\\n\")\n",
    "\n",
    "# Проверка на разные типы аффинитета в одной группе\n",
    "mixed_types = valid_stats[valid_stats['affinity_type_count'] > 1]\n",
    "if not mixed_types.empty:\n",
    "    print(f\"ВНИМАНИЕ: В {len(mixed_types)} группах смешаны разные типы аффинитета (сравнение может быть некорректным).\")\n",
    "else:\n",
    "    print(\"Типы аффинитета внутри каждой группы совпадают (корректно сравнивать числа).\\n\")\n",
    "\n",
    "# Топ-5 групп с самым большим разбросом\n",
    "print(\"=== ТОП-5 ГРУПП С САМЫМ БОЛЬШИМ РАЗБРОСОМ АФФИНИТЕТА ===\")\n",
    "top_diff_groups = valid_stats.nlargest(5, 'affinity_range')\n",
    "\n",
    "for idx, (indices, row) in enumerate(top_diff_groups.iterrows()):\n",
    "    heavy, light, antigen = indices\n",
    "    print(f\"\\nГруппа {idx+1} (разброс {row['affinity_range']:.4f}):\")\n",
    "    # Получаем строки этой группы\n",
    "    group_rows = analysis_df[\n",
    "        (analysis_df['heavy_sequence'] == heavy) & \n",
    "        (analysis_df['light_sequence'] == light) &\n",
    "        (analysis_df['antigen_sequence'] == antigen)\n",
    "    ]\n",
    "    display(group_rows[['dataset', 'affinity_type', 'affinity', 'confidence', 'processed_measurement']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb2693",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2080d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер до удаления дубликатов: (402392, 11)\n",
      "Размер после удаления дубликатов: (337196, 11)\n"
     ]
    }
   ],
   "source": [
    "# Удаляем дубликаты по heavy_sequence + light_sequence + antigen_sequence\n",
    "# Оставляем первую запись из каждой группы (affinity отличается, но это не важно)\n",
    "\n",
    "print(f\"Размер до удаления дубликатов: {agab_df.shape}\")\n",
    "\n",
    "agab_df = agab_df.drop_duplicates(\n",
    "    subset=['heavy_sequence', 'light_sequence', 'antigen_sequence'],\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(f\"Размер после удаления дубликатов: {agab_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf36bed",
   "metadata": {},
   "source": [
    "#### ANARCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "112571c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scFv всего: 53603\n",
      "оба домена уже раздельно: 0\n",
      "только heavy заполнен: 53603\n",
      "только light заполнен: 0\n",
      "оба пустые: 0\n"
     ]
    }
   ],
   "source": [
    "def is_empty(x):\n",
    "    return x is None or (isinstance(x, float) and np.isnan(x)) or (isinstance(x, str) and x.strip() == \"\")\n",
    "\n",
    "scfv_df = agab_df[agab_df['scfv'] == True].copy()\n",
    "\n",
    "# сколько scFv-строк уже имеют обе цепи\n",
    "both_present = (~scfv_df[\"heavy_sequence\"].apply(is_empty)) & (~scfv_df[\"light_sequence\"].apply(is_empty))\n",
    "only_heavy   = (~scfv_df[\"heavy_sequence\"].apply(is_empty)) & ( scfv_df[\"light_sequence\"].apply(is_empty))\n",
    "only_light   = ( scfv_df[\"heavy_sequence\"].apply(is_empty)) & (~scfv_df[\"light_sequence\"].apply(is_empty))\n",
    "\n",
    "print(\"scFv всего:\", len(scfv_df))\n",
    "print(\"оба домена уже раздельно:\", both_present.sum())\n",
    "print(\"только heavy заполнен:\", only_heavy.sum())\n",
    "print(\"только light заполнен:\", only_light.sum())\n",
    "print(\"оба пустые:\", ((~both_present) & (~only_heavy) & (~only_light)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47a3fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5ed7450bc5413ebfedceee4307fe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting sequences:   0%|          | 0/337196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ANARCI on 620789 sequences using 16 processes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6c43bb027a486492ada82af0def97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANARCI Parallel:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ANARCI results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6e8dd571c44f3f97f8473ee31a4c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing results:   0%|          | 0/620789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating DataFrame columns...\n",
      "Extraction complete. Found 337196 Heavy chains and 337196 Light chains.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path to import worker module\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "# Import worker function from external file to fix pickling error\n",
    "try:\n",
    "    from data.anarci_worker import run_anarci_batch\n",
    "except ImportError:\n",
    "    # Fallback if running from data directory directly\n",
    "    try:\n",
    "        from anarci_worker import run_anarci_batch\n",
    "    except ImportError:\n",
    "        print(\"Error: Could not import anarci_worker. Make sure data/anarci_worker.py exists.\")\n",
    "\n",
    "def process_antibody_sequences_optimized(df, n_jobs=8, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Optimized version of antibody processing using multiprocessing.\n",
    "    Runs ANARCI in parallel batches and minimizes DataFrame operations.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Prepare sequences list efficiently\n",
    "    print(\"Preparing sequences...\")\n",
    "    sequences_to_anarci = []\n",
    "    mapping_list = [] \n",
    "    \n",
    "    rows_iter = zip(df.index, df['scfv'], df['heavy_sequence'], df['light_sequence'])\n",
    "    \n",
    "    for idx, is_scfv, h_seq, l_seq in tqdm(rows_iter, total=len(df), desc=\"Collecting sequences\"):\n",
    "        if is_scfv:\n",
    "            seq = h_seq\n",
    "            if pd.isna(seq) or len(seq) == 0:\n",
    "                 seq = l_seq if isinstance(l_seq, str) else ''\n",
    "            \n",
    "            if isinstance(seq, str) and len(seq) > 0:\n",
    "                internal_id = f\"{idx}_scfv\"\n",
    "                sequences_to_anarci.append((internal_id, seq))\n",
    "                mapping_list.append((idx, 'scfv', seq))\n",
    "        else:\n",
    "            if isinstance(h_seq, str) and len(h_seq) > 0:\n",
    "                internal_id = f\"{idx}_heavy\"\n",
    "                sequences_to_anarci.append((internal_id, h_seq))\n",
    "                mapping_list.append((idx, 'heavy', h_seq))\n",
    "            \n",
    "            if isinstance(l_seq, str) and len(l_seq) > 0:\n",
    "                internal_id = f\"{idx}_light\"\n",
    "                sequences_to_anarci.append((internal_id, l_seq))\n",
    "                mapping_list.append((idx, 'light', l_seq))\n",
    "                \n",
    "    total_seqs = len(sequences_to_anarci)\n",
    "    print(f\"Running ANARCI on {total_seqs} sequences using {n_jobs} processes...\")\n",
    "    \n",
    "    # 2. Parallel execution\n",
    "    chunk_size = min(batch_size, max(1, total_seqs // (n_jobs * 2)))\n",
    "    chunks = [sequences_to_anarci[i : i + chunk_size] for i in range(0, total_seqs, chunk_size)]\n",
    "    \n",
    "    all_numbering = []\n",
    "    all_alignment_details = []\n",
    "    \n",
    "    with Pool(processes=n_jobs) as pool:\n",
    "        results = list(tqdm(pool.imap(run_anarci_batch, chunks), total=len(chunks), desc=\"ANARCI Parallel\"))\n",
    "    \n",
    "    # Unpack results\n",
    "    for num, aln, err in results:\n",
    "        if err and not isinstance(err, str): \n",
    "             pass\n",
    "        if num is None:\n",
    "            print(f\"Batch failed: {err}\")\n",
    "            continue \n",
    "        all_numbering.extend(num)\n",
    "        all_alignment_details.extend(aln)\n",
    "\n",
    "    # 3. Process results efficiently\n",
    "    print(\"Processing ANARCI results...\")\n",
    "    \n",
    "    heavy_seqs = {}\n",
    "    light_seqs = {}\n",
    "    heavy_specs = {}\n",
    "    light_specs = {}\n",
    "    heavy_germs = {}\n",
    "    light_germs = {}\n",
    "    heavy_nums = {}\n",
    "    light_nums = {}\n",
    "    \n",
    "    for i, (num_hits, hits) in tqdm(enumerate(zip(all_numbering, all_alignment_details)), total=len(all_numbering), desc=\"Parsing results\"):\n",
    "        if not hits or not num_hits: \n",
    "            continue\n",
    "            \n",
    "        original_idx, seq_type, full_seq = mapping_list[i]\n",
    "        \n",
    "        for domain_idx, hit in enumerate(hits):\n",
    "            if not hit: continue\n",
    "            \n",
    "            chain_type = hit.get('chain_type', 'unknown')\n",
    "            start = hit.get('query_start')\n",
    "            end = hit.get('query_end')\n",
    "            species = hit.get('species')\n",
    "            germlines = hit.get('germlines')\n",
    "            \n",
    "            domain_data = num_hits[domain_idx]\n",
    "            numbering_json = None\n",
    "            if domain_data:\n",
    "                residues_json = [\n",
    "                    {\"pos\": r[0][0], \"ins\": r[0][1].strip(), \"aa\": r[1]}\n",
    "                    for r in domain_data[0]\n",
    "                ]\n",
    "                numbering_obj = {\n",
    "                    \"domain_start\": domain_data[1],\n",
    "                    \"domain_end\": domain_data[2],\n",
    "                    \"residues\": residues_json\n",
    "                }\n",
    "                numbering_json = json.dumps([numbering_obj])\n",
    "            \n",
    "            domain_seq = full_seq[start : end] if (start is not None and end is not None) else None\n",
    "\n",
    "            is_heavy = False\n",
    "            if seq_type == 'scfv':\n",
    "                if chain_type == 'H': is_heavy = True\n",
    "                elif chain_type in ['K', 'L']: is_heavy = False\n",
    "                else: continue\n",
    "            elif seq_type == 'heavy':\n",
    "                if chain_type == 'H': is_heavy = True\n",
    "                else: continue\n",
    "            elif seq_type == 'light':\n",
    "                if chain_type in ['K', 'L']: is_heavy = False\n",
    "                else: continue\n",
    "            \n",
    "            if is_heavy:\n",
    "                heavy_seqs[original_idx] = domain_seq\n",
    "                heavy_specs[original_idx] = species\n",
    "                heavy_germs[original_idx] = str(germlines)\n",
    "                heavy_nums[original_idx] = numbering_json\n",
    "            else:\n",
    "                light_seqs[original_idx] = domain_seq\n",
    "                light_specs[original_idx] = species\n",
    "                light_germs[original_idx] = str(germlines)\n",
    "                light_nums[original_idx] = numbering_json\n",
    "\n",
    "    print(\"Updating DataFrame columns...\")\n",
    "    \n",
    "    # Convert dictionaries to Series once\n",
    "    heavy_seqs_series = pd.Series(heavy_seqs, name='heavy_sequence')\n",
    "    light_seqs_series = pd.Series(light_seqs, name='light_sequence')\n",
    "    \n",
    "    # Use combine_first to update only found values\n",
    "    # heavy_seqs_series contains only updated sequences, combine_first fills gaps from original\n",
    "    df['heavy_sequence'] = heavy_seqs_series.combine_first(df['heavy_sequence'])\n",
    "    df['light_sequence'] = light_seqs_series.combine_first(df['light_sequence'])\n",
    "    \n",
    "    # For new columns, map is fine as they are empty or overwritten\n",
    "    df['heavy_species'] = df.index.map(heavy_specs)\n",
    "    df['light_species'] = df.index.map(light_specs)\n",
    "    df['heavy_germlines'] = df.index.map(heavy_germs)\n",
    "    df['light_germlines'] = df.index.map(light_germs)\n",
    "    df['heavy_numbering'] = df.index.map(heavy_nums)\n",
    "    df['light_numbering'] = df.index.map(light_nums)\n",
    "    found_h = df['heavy_sequence'].notna().sum()\n",
    "    found_l = df['light_sequence'].notna().sum()\n",
    "    print(f\"Extraction complete. Found {found_h} Heavy chains and {found_l} Light chains.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the optimized function\n",
    "if __name__ == '__main__':\n",
    "    agab_df = process_antibody_sequences_optimized(agab_df, n_jobs=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432b436",
   "metadata": {},
   "source": [
    "### Проверка ANARCI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37a4bf",
   "metadata": {},
   "source": [
    "### Save to agab.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80ed7ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отфильтрованные данные сохранены в agab.parquet\n",
      "Размер сохраненных данных: (337196, 17)\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем датафрейм для использования в других ноутбуках\n",
    "output_path = 'agab.parquet'\n",
    "\n",
    "# Fix mixed types for Parquet export\n",
    "# 'affinity' contains both numbers and strings (e.g. 'h'), so we must save as string\n",
    "agab_df['affinity'] = agab_df['affinity'].astype(str)\n",
    "agab_df['processed_measurement'] = agab_df['processed_measurement'].astype(str)\n",
    "\n",
    "agab_df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "print(f\"Отфильтрованные данные сохранены в {output_path}\")\n",
    "print(f\"Размер сохраненных данных: {agab_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_parquet('agab.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Файл agab.parquet не найден, использую текущий agab_df из памяти (если есть)\")\n",
    "    if 'agab_df' in locals():\n",
    "        df = agab_df\n",
    "    else:\n",
    "        raise ValueError(\"Загрузите датафрейм в переменную df\")\n",
    "\n",
    "print(f\"Всего записей: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98efb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРОВЕРКА РЕЗУЛЬТАТОВ ОБРАБОТКИ (agab_df_test) ===\n",
      "\n",
      "[OK] Все новые колонки созданы: ['heavy_species', 'light_species', 'heavy_germlines', 'light_germlines', 'heavy_numbering', 'light_numbering']\n",
      "\n",
      "--- Статистика заполнения ---\n",
      "heavy_species: 337196/337196 (100.0%)\n",
      "light_species: 337196/337196 (100.0%)\n",
      "heavy_germlines: 337196/337196 (100.0%)\n",
      "light_germlines: 337196/337196 (100.0%)\n",
      "heavy_numbering: 337196/337196 (100.0%)\n",
      "light_numbering: 337196/337196 (100.0%)\n",
      "heavy_sequence: 337196/337196 (100.0%)\n",
      "light_sequence: 337196/337196 (100.0%)\n",
      "\n",
      "--- Проверка формата JSON (первые 3 непустых) ---\n",
      "[OK] Row 3 heavy_numbering valid JSON. Domains: 1\n",
      "     First domain residues count: 128\n",
      "[OK] Row 5 heavy_numbering valid JSON. Domains: 1\n",
      "     First domain residues count: 128\n",
      "[OK] Row 12 heavy_numbering valid JSON. Domains: 1\n",
      "     First domain residues count: 128\n",
      "\n",
      "--- Проверка разделения scFv ---\n",
      "Всего scFv: 53603\n",
      "scFv с обеими цепями (H+L): 53603 (100.0%)\n",
      "scFv только с Heavy: 0 (0.0%)\n",
      "scFv только с Light: 0 (0.0%)\n",
      "\n",
      "--- Пример одной записи (Heavy) ---\n",
      "Species: mouse\n",
      "Germlines: {'v_gene': [('human', 'IGHV1-46*01'), 0.8571428571428571], 'j_gene': [('human', 'IGHJ4*01'), 0.8571428571428571]}\n",
      "Sequence (trimmed): QVQLVQSGAEVKKPGASVKV...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ПРОВЕРКА РЕЗУЛЬТАТОВ ОБРАБОТКИ (agab_df_test) ===\\n\")\n",
    "\n",
    "# 1. Проверка наличия новых колонок\n",
    "expected_cols = [\n",
    "    'heavy_species', 'light_species',\n",
    "    'heavy_germlines', 'light_germlines',\n",
    "    'heavy_numbering', 'light_numbering'\n",
    "]\n",
    "missing_cols = [c for c in expected_cols if c not in agab_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"[ОШИБКА] Отсутствуют колонки: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"[OK] Все новые колонки созданы: {expected_cols}\")\n",
    "\n",
    "# 2. Проверка заполнения (непустые значения)\n",
    "print(\"\\n--- Статистика заполнения ---\")\n",
    "total_rows = len(agab_df)\n",
    "if total_rows > 0:\n",
    "    for col in expected_cols + ['heavy_sequence', 'light_sequence']:\n",
    "        filled = agab_df[col].notna().sum()\n",
    "        print(f\"{col}: {filled}/{total_rows} ({filled/total_rows*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Датафрейм пуст!\")\n",
    "\n",
    "# 3. Проверка формата JSON в numbering\n",
    "print(\"\\n--- Проверка формата JSON (первые 3 непустых) ---\")\n",
    "sample_h = agab_df[agab_df['heavy_numbering'].notna()].head(3)\n",
    "for idx, row in sample_h.iterrows():\n",
    "    try:\n",
    "        data = json.loads(row['heavy_numbering'])\n",
    "        print(f\"[OK] Row {idx} heavy_numbering valid JSON. Domains: {len(data)}\")\n",
    "        if data:\n",
    "             print(f\"     First domain residues count: {len(data[0].get('residues', []))}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ОШИБКА] Row {idx} heavy_numbering invalid JSON: {e}\")\n",
    "\n",
    "# 4. Проверка разделения scFv\n",
    "print(\"\\n--- Проверка разделения scFv ---\")\n",
    "scfv_rows = agab_df[agab_df['scfv'] == True]\n",
    "if len(scfv_rows) > 0:\n",
    "    scfv_with_chains = scfv_rows[\n",
    "        scfv_rows['heavy_sequence'].notna() & \n",
    "        scfv_rows['light_sequence'].notna()\n",
    "    ]\n",
    "    scfv_only_heavy = scfv_rows[\n",
    "        scfv_rows['heavy_sequence'].notna() & \n",
    "        scfv_rows['light_sequence'].isna()\n",
    "    ]\n",
    "    scfv_only_light = scfv_rows[\n",
    "        scfv_rows['heavy_sequence'].isna() & \n",
    "        scfv_rows['light_sequence'].notna()\n",
    "    ]\n",
    "    print(f\"Всего scFv: {len(scfv_rows)}\")\n",
    "    print(f\"scFv с обеими цепями (H+L): {len(scfv_with_chains)} ({len(scfv_with_chains)/len(scfv_rows)*100:.1f}%)\")\n",
    "    print(f\"scFv только с Heavy: {len(scfv_only_heavy)} ({len(scfv_only_heavy)/len(scfv_rows)*100:.1f}%)\")\n",
    "    print(f\"scFv только с Light: {len(scfv_only_light)} ({len(scfv_only_light)/len(scfv_rows)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"В датафрейме нет записей scFv.\")\n",
    "\n",
    "# 5. Пример данных\n",
    "print(\"\\n--- Пример одной записи (Heavy) ---\")\n",
    "if not sample_h.empty:\n",
    "    example_row = sample_h.iloc[0]\n",
    "    print(f\"Species: {example_row['heavy_species']}\")\n",
    "    print(f\"Germlines: {example_row['heavy_germlines']}\")\n",
    "    print(f\"Sequence (trimmed): {example_row['heavy_sequence'][:20]}...\")\n",
    "else:\n",
    "    print(\"Нет обработанных heavy chains для показа примера.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для безопасного извлечения длины\n",
    "def get_len(seq):\n",
    "    return len(seq) if isinstance(seq, str) else 0\n",
    "\n",
    "df['heavy_len'] = df['heavy_sequence'].apply(get_len)\n",
    "df['light_len'] = df['light_sequence'].apply(get_len)\n",
    "\n",
    "# --- ПРОВЕРКА 1: Распределение длин ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Heavy chain length\n",
    "h_lens = df[df['heavy_len'] > 0]['heavy_len']\n",
    "sns.histplot(h_lens, bins=range(80, 160), ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(f'Heavy Chain Lengths (Mean: {h_lens.mean():.1f})')\n",
    "axes[0].axvline(105, color='r', linestyle='--', alpha=0.5, label='Min Exp (105)')\n",
    "axes[0].axvline(135, color='r', linestyle='--', alpha=0.5, label='Max Exp (135)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Light chain length\n",
    "l_lens = df[df['light_len'] > 0]['light_len']\n",
    "sns.histplot(l_lens, bins=range(80, 160), ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title(f'Light Chain Lengths (Mean: {l_lens.mean():.1f})')\n",
    "axes[1].axvline(100, color='r', linestyle='--', alpha=0.5, label='Min Exp (100)')\n",
    "axes[1].axvline(125, color='r', linestyle='--', alpha=0.5, label='Max Exp (125)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Выбросы\n",
    "print(\"\\n=== Выбросы по длине ===\")\n",
    "print(f\"Heavy < 90 aa: {len(h_lens[h_lens < 90])} шт.\")\n",
    "print(f\"Heavy > 140 aa: {len(h_lens[h_lens > 140])} шт.\")\n",
    "print(f\"Light < 90 aa: {len(l_lens[l_lens < 90])} шт.\")\n",
    "print(f\"Light > 135 aa: {len(l_lens[l_lens > 135])} шт.\")\n",
    "\n",
    "# --- ПРОВЕРКА 2: Start/End паттерны ---\n",
    "def get_top_kmers(sequences, k=5, n=10, side='start'):\n",
    "    seqs = [s for s in sequences if isinstance(s, str) and len(s) >= k]\n",
    "    if side == 'start':\n",
    "        kmers = [s[:k] for s in seqs]\n",
    "    else:\n",
    "        kmers = [s[-k:] for s in seqs]\n",
    "    return Counter(kmers).most_common(n)\n",
    "\n",
    "print(\"\\n=== Топ-10 паттернов начала и конца (Heavy) ===\")\n",
    "print(\"Start:\", get_top_kmers(df['heavy_sequence'], side='start'))\n",
    "print(\"End:  \", get_top_kmers(df['heavy_sequence'], side='end'))\n",
    "\n",
    "print(\"\\n=== Топ-10 паттернов начала и конца (Light) ===\")\n",
    "print(\"Start:\", get_top_kmers(df['light_sequence'], side='start'))\n",
    "print(\"End:  \", get_top_kmers(df['light_sequence'], side='end'))\n",
    "\n",
    "# --- ПРОВЕРКА 3: Консервативные цистеины (из numbering) ---\n",
    "print(\"\\n=== Проверка консервативных цистеинов (C23, C104) ===\")\n",
    "\n",
    "def check_cysteines(numbering_json):\n",
    "    if not numbering_json or pd.isna(numbering_json):\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(numbering_json)\n",
    "        if not data: return False\n",
    "        \n",
    "        # Получаем список остатков первого домена\n",
    "        residues = data[0].get('residues', [])\n",
    "        \n",
    "        has_c23 = False\n",
    "        has_c104 = False\n",
    "        \n",
    "        for r in residues:\n",
    "            # r['pos'] это номер позиции IMGT\n",
    "            # r['aa'] это аминокислота\n",
    "            pos = r.get('pos')\n",
    "            aa = r.get('aa')\n",
    "            \n",
    "            if pos == 23 and aa == 'C':\n",
    "                has_c23 = True\n",
    "            if pos == 104 and aa == 'C':\n",
    "                has_c104 = True\n",
    "                \n",
    "        return has_c23 and has_c104\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['heavy_cys_ok'] = df['heavy_numbering'].apply(check_cysteines)\n",
    "df['light_cys_ok'] = df['light_numbering'].apply(check_cysteines)\n",
    "\n",
    "print(f\"Heavy Chains with both C23 & C104: {df['heavy_cys_ok'].sum()} / {df['heavy_numbering'].notna().sum()}\")\n",
    "print(f\"Light Chains with both C23 & C104: {df['light_cys_ok'].sum()} / {df['light_numbering'].notna().sum()}\")\n",
    "\n",
    "# --- ПРОВЕРКА 4: Зависимость от вида (Species) ---\n",
    "print(\"\\n=== Средняя длина по видам (Top 5) ===\")\n",
    "top_species = df['heavy_species'].value_counts().head(5).index\n",
    "print(df[df['heavy_species'].isin(top_species)].groupby('heavy_species')['heavy_len'].agg(['mean', 'std', 'count']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antibody_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
