{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9650f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 20 parquet файлов\n",
      "Общий размер данных: (1227083, 11)\n",
      "Колонки: ['dataset', 'heavy_sequence', 'light_sequence', 'scfv', 'affinity_type', 'affinity', 'antigen_sequence', 'confidence', 'nanobody', 'metadata', 'processed_measurement']\n"
     ]
    }
   ],
   "source": [
    "def load_asd_data_with_pandas(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загружает все parquet файлы из папки asd в один pandas DataFrame\n",
    "\n",
    "    Args:\n",
    "        data_path: путь к папке с данными\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: объединенный DataFrame со всеми данными\n",
    "    \"\"\"\n",
    "    # Получаем все parquet файлы из папки\n",
    "    parquet_files = glob.glob(os.path.join(data_path, \"part-*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise ValueError(f\"Не найдено parquet файлов в папке {data_path}\")\n",
    "\n",
    "    print(f\"Найдено {len(parquet_files)} parquet файлов\")\n",
    "\n",
    "    # Загружаем все файлы в список DataFrame'ов\n",
    "    dataframes = []\n",
    "    for file_path in parquet_files:\n",
    "        # print(f\"Загружаем файл: {os.path.basename(file_path)}\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Объединяем все DataFrame'ы в один\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    print(f\"Общий размер данных: {combined_df.shape}\")\n",
    "    print(f\"Колонки: {list(combined_df.columns)}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Загружаем данные\n",
    "agab_df = load_asd_data_with_pandas('./asd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ff1b4",
   "metadata": {},
   "source": [
    "### По файлам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc456a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== АНАЛИЗ ДУБЛИКАТОВ ПО ФАЙЛАМ ===\n",
      "\n",
      "Дубликаты внутри каждого файла:\n",
      "                                                               file  rows  duplicates  duplicate_pct\n",
      "part-00000-3a065afd-b2fa-4875-a6e5-911e95e3f86c-c000.snappy.parquet 61329        2777       4.528037\n",
      "part-00001-74fdee0d-8448-4b0a-921c-f1f0ef356cdf-c000.snappy.parquet 60919        2740       4.497776\n",
      "part-00002-634ededa-5adf-4170-93ba-2dac2bd74705-c000.snappy.parquet 63736        2550       4.000879\n",
      "part-00003-427bc79e-0e40-4a8c-a2be-d0fbe09f03c0-c000.snappy.parquet 61152        2542       4.156855\n",
      "part-00004-eb3dc336-995c-48bd-840f-49d411a89b8e-c000.snappy.parquet 60125        2936       4.883160\n",
      "part-00005-846a5164-9ca8-4438-af5f-07ad5348f327-c000.snappy.parquet 61104        2816       4.608536\n",
      "part-00006-b818506c-2926-406c-936b-66da5c9acdbc-c000.snappy.parquet 62158        3105       4.995334\n",
      "part-00007-7ab8e466-84f8-43e0-8874-9c1bbf210d4e-c000.snappy.parquet 60256        2778       4.610329\n",
      "part-00008-397aa529-5cb9-4e24-8898-c9940200ae64-c000.snappy.parquet 60278        2860       4.744683\n",
      "part-00009-6b319ece-d8eb-4e15-b579-4e98d3a456a1-c000.snappy.parquet 60758        2858       4.703907\n",
      "part-00010-7edd7ab3-f323-4718-a2ac-138fb65b3f42-c000.snappy.parquet 62283        2811       4.513270\n",
      "part-00011-86ada209-259a-4a93-8a61-ee6555e0f25d-c000.snappy.parquet 61004        2713       4.447249\n",
      "part-00012-fe431735-b7e1-4367-b665-15a59e7bd12c-c000.snappy.parquet 59117        2893       4.893685\n",
      "part-00013-26624da0-a286-49c6-98c5-c019816424b8-c000.snappy.parquet 61156        2738       4.477075\n",
      "part-00014-0bbd5d34-4a5c-4c7f-9c69-ba69e33861b1-c000.snappy.parquet 61704        3064       4.965642\n",
      "part-00015-af04209c-672a-4fd7-9cc5-1fdaeaf06aa0-c000.snappy.parquet 62936        2980       4.734969\n",
      "part-00016-883dd12e-3f06-4505-b326-04b6c16a7852-c000.snappy.parquet 62627        2804       4.477302\n",
      "part-00017-b88913fa-e655-4662-8208-45e9f4d38488-c000.snappy.parquet 61052        2785       4.561685\n",
      "part-00018-630653e8-04e5-4d69-bd4f-96225f04fb82-c000.snappy.parquet 61479        2896       4.710552\n",
      "part-00019-2a17653c-3a60-4f9a-b840-e7b168d3d6f9-c000.snappy.parquet 61910        2790       4.506542\n",
      "\n",
      "Общее количество дубликатов внутри файлов: 56436\n"
     ]
    }
   ],
   "source": [
    "print(\"=== АНАЛИЗ ДУБЛИКАТОВ ПО ФАЙЛАМ ===\\n\")\n",
    "\n",
    "parquet_files = glob.glob(os.path.join('./asd', \"part-*.parquet\"))\n",
    "parquet_files.sort()\n",
    "\n",
    "file_duplicates = []\n",
    "for file_path in parquet_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    duplicates_in_file = df.drop(columns=['metadata']).duplicated().sum()\n",
    "    file_duplicates.append({\n",
    "        'file': file_name,\n",
    "        'rows': len(df),\n",
    "        'duplicates': duplicates_in_file,\n",
    "        'duplicate_pct': duplicates_in_file / len(df) * 100 if len(df) > 0 else 0\n",
    "    })\n",
    "\n",
    "file_duplicates_df = pd.DataFrame(file_duplicates)\n",
    "print(\"Дубликаты внутри каждого файла:\")\n",
    "print(file_duplicates_df.to_string(index=False))\n",
    "print(f\"\\nОбщее количество дубликатов внутри файлов: {file_duplicates_df['duplicates'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d319ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== АНАЛИЗ ПЕРЕКРЫТИЙ МЕЖДУ ФАЙЛАМИ ===\n",
      "\n",
      "Перекрытий между файлами не обнаружено.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== АНАЛИЗ ПЕРЕКРЫТИЙ МЕЖДУ ФАЙЛАМИ ===\\n\")\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "file_unique_sets = {os.path.basename(fp): set(pd.read_parquet(fp).drop(columns=['metadata']).apply(tuple, axis=1)) for fp in parquet_files}\n",
    "\n",
    "overlaps = [{'file1': f1, 'file2': f2, 'overlap_count': overlap, 'file1_size': len(s1), 'file2_size': len(s2), \n",
    "             'overlap_pct_file1': overlap / len(s1) * 100 if len(s1) > 0 else 0, 'overlap_pct_file2': overlap / len(s2) * 100 if len(s2) > 0 else 0}\n",
    "            for (f1, s1), (f2, s2) in combinations(file_unique_sets.items(), 2) if (overlap := len(s1 & s2)) > 0]\n",
    "\n",
    "if overlaps:\n",
    "    overlaps_df = pd.DataFrame(overlaps)\n",
    "    print(\"Найдены перекрытия между файлами:\")\n",
    "    print(overlaps_df.to_string(index=False))\n",
    "    print(f\"\\nВсего пар файлов с перекрытиями: {len(overlaps)}\")\n",
    "    print(f\"Общее количество перекрывающихся записей: {overlaps_df['overlap_count'].sum()}\")\n",
    "else:\n",
    "    print(\"Перекрытий между файлами не обнаружено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b05fdd",
   "metadata": {},
   "source": [
    "### По датасетам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== АНАЛИЗ ДУБЛИКАТОВ ПО ДАТАСЕТАМ ===\n",
      "\n",
      "Дубликаты по датасетам:\n",
      "                dataset  total_rows  duplicates  duplicate_pct  unique_rows\n",
      "               covid-19       54625       27482      50.310297        27143\n",
      "                    hiv       48008       24715      51.481003        23293\n",
      "                 biomap        2725        1402      51.449541         1323\n",
      "  structures-antibodies        2711        1170      43.157506         1541\n",
      "                genbank        2989         717      23.987956         2272\n",
      "  structures-nanobodies        1258         649      51.589825          609\n",
      "                patents      217463         164       0.075415       217299\n",
      "                   abbd      155853         132       0.084695       155721\n",
      "               alphaseq      198703           4       0.002013       198699\n",
      "flab_shanehsazzadeh2023         446           1       0.224215          445\n",
      "               abdesign         672           0       0.000000          672\n",
      "        flab_koenig2017        4275           0       0.000000         4275\n",
      "                    met        4000           0       0.000000         4000\n",
      "        flab_rosace2023          25           0       0.000000           25\n",
      "           flab_hie2022          55           0       0.000000           55\n",
      "                   aatp          93           0       0.000000           93\n",
      "    flab_warszawski2019        2048           0       0.000000         2048\n",
      "                ab-bind         283           0       0.000000          283\n",
      "                   rmna          10           0       0.000000           10\n",
      "                    aae          35           0       0.000000           35\n",
      "                   buzz      524346           0       0.000000       524346\n",
      "                    osh          30           0       0.000000           30\n",
      "                   dlgo         360           0       0.000000          360\n",
      "               skempiv2         434           0       0.000000          434\n",
      "             literature        5636           0       0.000000         5636\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== АНАЛИЗ ДУБЛИКАТОВ ПО ДАТАСЕТАМ ===\\n\")\n",
    "\n",
    "dataset_analysis = []\n",
    "for dataset in agab_df['dataset'].unique():\n",
    "    dataset_df = agab_df[agab_df['dataset'] == dataset]\n",
    "    duplicates = dataset_df.drop(columns=['metadata']).duplicated().sum()\n",
    "    dataset_analysis.append({\n",
    "        'dataset': dataset,\n",
    "        'total_rows': len(dataset_df),\n",
    "        'duplicates': duplicates,\n",
    "        'duplicate_pct': duplicates / len(dataset_df) * 100 if len(dataset_df) > 0 else 0,\n",
    "        'unique_rows': len(dataset_df) - duplicates\n",
    "    })\n",
    "\n",
    "dataset_analysis_df = pd.DataFrame(dataset_analysis).sort_values('duplicates', ascending=False)\n",
    "print(\"Дубликаты по датасетам:\")\n",
    "print(dataset_analysis_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a270366",
   "metadata": {},
   "source": [
    "### Cтатистика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15afe93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Статистика дубликатов ===\n",
      "Всего строк: 1227083\n",
      "Дубликатов: 56436\n",
      "Уникальных строк: 1170647\n",
      "\n",
      "=== Дубликаты с учетом metadata ===\n",
      "Дубликатов с учетом metadata: 28602\n",
      "Уникальных строк с учетом metadata: 1198481\n",
      "\n",
      "=== Сравнительная статистика ===\n",
      "Разница в количестве дубликатов: 27834\n",
      "Metadata влияет на 27834 записей\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"=== Статистика дубликатов ===\")\n",
    "print(f\"Всего строк: {len(agab_df)}\")\n",
    "\n",
    "# Создаем маску дубликатов один раз для эффективности\n",
    "duplicates_mask = agab_df.drop(columns='metadata').duplicated()\n",
    "duplicates_count = duplicates_mask.sum()\n",
    "\n",
    "print(f\"Дубликатов: {duplicates_count}\")\n",
    "print(f\"Уникальных строк: {len(agab_df) - duplicates_count}\")\n",
    "\n",
    "# Дубликаты с учетом столбца metadata в JSON\n",
    "print(\"\\n=== Дубликаты с учетом metadata ===\")\n",
    "\n",
    "# Создаем временный DataFrame, преобразуя metadata в строку JSON\n",
    "temp_df = agab_df.copy()\n",
    "temp_df['metadata_json'] = temp_df['metadata'].apply(lambda x: json.dumps(x, sort_keys=True) if isinstance(x, dict) else str(x))\n",
    "\n",
    "# Проверяем дубликаты, исключая столбец metadata (используем все столбцы кроме metadata)\n",
    "columns_to_check = [col for col in temp_df.columns if col != 'metadata']\n",
    "duplicates_with_metadata_mask = temp_df[columns_to_check].duplicated()\n",
    "duplicates_with_metadata_count = duplicates_with_metadata_mask.sum()\n",
    "\n",
    "print(f\"Дубликатов с учетом metadata: {duplicates_with_metadata_count}\")\n",
    "print(f\"Уникальных строк с учетом metadata: {len(agab_df) - duplicates_with_metadata_count}\")\n",
    "\n",
    "# Сравнительная статистика\n",
    "print(\"\\n=== Сравнительная статистика ===\")\n",
    "print(f\"Разница в количестве дубликатов: {duplicates_count - duplicates_with_metadata_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2988ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Распределение дубликатов по dataset (без учета metadata) ===\n",
      "\n",
      "Топ-10 dataset по количеству дубликатов (без metadata):\n",
      "                dataset  total_rows  duplicates_count  duplicates_ratio\n",
      "               covid-19       54625             27482            0.5031\n",
      "                    hiv       48008             24715            0.5148\n",
      "                 biomap        2725              1402            0.5145\n",
      "  structures-antibodies        2711              1170            0.4316\n",
      "                genbank        2989               717            0.2399\n",
      "  structures-nanobodies        1258               649            0.5159\n",
      "                patents      217463               164            0.0008\n",
      "                   abbd      155853               132            0.0008\n",
      "               alphaseq      198703                 4            0.0000\n",
      "flab_shanehsazzadeh2023         446                 1            0.0022\n",
      "\n",
      "Всего dataset: 25\n",
      "Dataset с дубликатами: 10\n",
      "Dataset без дубликатов: 15\n",
      "\n",
      "Максимальное количество дубликатов в одном dataset: 27482\n",
      "Среднее количество дубликатов на dataset: 2257.4\n",
      "Медианное количество дубликатов на dataset: 0.0\n",
      "\n",
      "Dataset с наибольшим процентом дубликатов: structures-nanobodies\n",
      "  - Дубликаты: 649/1258 (51.6%)\n"
     ]
    }
   ],
   "source": [
    "# Распределение дубликатов по dataset (без учета metadata)\n",
    "print(\"\\n=== Распределение дубликатов по dataset (без учета metadata) ===\")\n",
    "\n",
    "# Добавляем флаг дубликатов в исходный DataFrame (без metadata)\n",
    "agab_df['is_duplicate_no_metadata'] = duplicates_mask\n",
    "\n",
    "# Группируем по dataset и считаем статистику\n",
    "dataset_duplicates_no_metadata = agab_df.groupby('dataset').agg({\n",
    "    'is_duplicate_no_metadata': ['count', 'sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "# Переименовываем столбцы для удобства\n",
    "dataset_duplicates_no_metadata.columns = ['total_rows', 'duplicates_count', 'duplicates_ratio']\n",
    "dataset_duplicates_no_metadata = dataset_duplicates_no_metadata.reset_index()\n",
    "\n",
    "# Сортируем по количеству дубликатов\n",
    "dataset_duplicates_no_metadata = dataset_duplicates_no_metadata.sort_values('duplicates_count', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-10 dataset по количеству дубликатов (без metadata):\")\n",
    "print(dataset_duplicates_no_metadata.head(10).to_string(index=False))\n",
    "\n",
    "# Общая статистика по распределению\n",
    "print(f\"\\nВсего dataset: {len(dataset_duplicates_no_metadata)}\")\n",
    "print(f\"Dataset с дубликатами: {len(dataset_duplicates_no_metadata[dataset_duplicates_no_metadata['duplicates_count'] > 0])}\")\n",
    "print(f\"Dataset без дубликатов: {len(dataset_duplicates_no_metadata[dataset_duplicates_no_metadata['duplicates_count'] == 0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d61defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Распределение дубликатов по dataset (с учетом metadata) ===\n",
      "\n",
      "Топ-10 dataset по количеству дубликатов:\n",
      "                dataset  total_rows  duplicates_count  duplicates_ratio\n",
      "                    hiv       48008             24715            0.5148\n",
      "                 biomap        2725              1402            0.5145\n",
      "  structures-antibodies        2711               999            0.3685\n",
      "  structures-nanobodies        1258               566            0.4499\n",
      "                genbank        2989               480            0.1606\n",
      "               covid-19       54625               303            0.0055\n",
      "                   abbd      155853               132            0.0008\n",
      "               alphaseq      198703                 4            0.0000\n",
      "flab_shanehsazzadeh2023         446                 1            0.0022\n",
      "               skempiv2         434                 0            0.0000\n",
      "\n",
      "Всего dataset: 25\n",
      "Dataset с дубликатами: 9\n",
      "Dataset без дубликатов: 16\n"
     ]
    }
   ],
   "source": [
    "# Распределение дубликатов по dataset\n",
    "print(\"\\n=== Распределение дубликатов по dataset (с учетом metadata) ===\")\n",
    "\n",
    "# Добавляем флаг дубликатов в исходный DataFrame\n",
    "agab_df['is_duplicate'] = duplicates_with_metadata_mask\n",
    "\n",
    "# Группируем по dataset и считаем статистику\n",
    "dataset_duplicates = agab_df.groupby('dataset').agg({\n",
    "    'is_duplicate': ['count', 'sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "# Переименовываем столбцы для удобства\n",
    "dataset_duplicates.columns = ['total_rows', 'duplicates_count', 'duplicates_ratio']\n",
    "dataset_duplicates = dataset_duplicates.reset_index()\n",
    "\n",
    "# Сортируем по количеству дубликатов\n",
    "dataset_duplicates = dataset_duplicates.sort_values('duplicates_count', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-10 dataset по количеству дубликатов:\")\n",
    "print(dataset_duplicates.head(10).to_string(index=False))\n",
    "\n",
    "# Общая статистика по распределению\n",
    "print(f\"\\nВсего dataset: {len(dataset_duplicates)}\")\n",
    "print(f\"Dataset с дубликатами: {len(dataset_duplicates[dataset_duplicates['duplicates_count'] > 0])}\")\n",
    "print(f\"Dataset без дубликатов: {len(dataset_duplicates[dataset_duplicates['duplicates_count'] == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79073bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
